# Steps to follow for PySpark Data Stream execution:

1. Create <.py> file consists of Spark-Streaming code in Python (for DStream with textfile-input).

2. Create <.properties> Flume file for batch processing of 'errorlogs'.

3. Create directory structure at local and hdfs as defined in <.properties> file. Ensure that no other files are present at these folder locations.

4. Run flume agent in Unix-box console.

5. Create 'errorlogs' file and put it in local directory.
Note - Practically such files will be automatically copied to mentioned location from any process output at regular interval and since flume agent is running that file will be automatically copied to hdfs storage at the same time.

6. Run <.py> streaming code file in another Unix-box console.

7. Verify that streaming starts and we get output in terms of error words and count at console on the basis of  time interval in seconds given in streaming context of <.py> file.

8. Repeat step (5) at least 2-3 times and see the stream data count changes in console running <.py> file.